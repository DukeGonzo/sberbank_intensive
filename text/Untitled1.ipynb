{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10, mnist, imdb\n",
    "from random import randrange\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers, models, optimizers, backend, metrics, callbacks\n",
    "import codecs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 12) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "with open('labeledTrainData.tsv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    _ = next(reader)\n",
    "    for l in reader:\n",
    "        texts.append(l[2])\n",
    "        labels.append(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    w2i = None\n",
    "    i2w = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(filename, embedding_size=300):\n",
    "    vocabulary_mapping = {'PAD': 0}\n",
    "    temp = np.zeros(embedding_size)\n",
    "    embeddings_list = [[0] * embedding_size]\n",
    "    id = 1\n",
    "    with codecs.open(filename, 'rb', 'utf-8') as glove_file:\n",
    "        for line in glove_file:\n",
    "            items = line.strip().split()\n",
    "\n",
    "            if len(items) != embedding_size + 1:\n",
    "                continue\n",
    "\n",
    "            token = items[0]\n",
    "            embedding = np.array([float(i) for i in items[1:]])\n",
    "            temp += embedding\n",
    "            embeddings_list.append(embedding)\n",
    "            vocabulary_mapping[token] = id\n",
    "            id += 1\n",
    "\n",
    "    vocabulary_mapping['<UNK>'] = id\n",
    "    temp /= len(embeddings_list) - 2\n",
    "    embeddings_list.append(temp)\n",
    "    embeddings = np.array(embeddings_list)\n",
    "    vocab = Vocab()\n",
    "    vocab.i2w = {v: k for k, v in vocabulary_mapping.items()}\n",
    "    vocab.w2i = vocabulary_mapping\n",
    "    print('glove was loaded')\n",
    "    return embeddings, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove was loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings, vocab = load_glove('/mnt/Science/glove/glove.6B.100d.txt', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_to_id_vec(sent, vocab, max_len=42, mode='tokenize'):\n",
    "    sent = sent.lower()\n",
    "\n",
    "    if mode == 'tokenize':\n",
    "        tokens = word_tokenize(sent)\n",
    "    elif mode == 'split':\n",
    "        tokens = sent.split()\n",
    "    else:\n",
    "        raise Error()\n",
    "\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = (t for i, t in enumerate(tokens) if i < max_len)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in vocab.w2i:\n",
    "            result.append(vocab.w2i[token])\n",
    "        else:\n",
    "            result.append(vocab.w2i['<UNK>'])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for t in texts:\n",
    "    temp = sent_to_id_vec(t, vocab)\n",
    "    sequences.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
